{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Kanye lyrics with RNN\n",
        "With this project i attempt to create authentic Kanye lyrics with an RNN.\n",
        "* The RNN is trained on a Kaggle dataset of Kanye lyrics: https://www.kaggle.com/datasets/convolutionalnn/kanye-west-lyrics-dataset\n",
        "* The method is based on the TensorFlow Shakespeare text generation tutorial: https://www.tensorflow.org/text/tutorials/text_generation"
      ],
      "metadata": {
        "id": "OaUMiAnbAO-T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "tIxEDICP5JN0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compatibility\n",
        "text = open(\"/content/Kanye West Lyrics.txt\", \"rb\").read().decode(encoding=\"utf-8\")\n",
        "# lenght of text is the number of charecters in it\n",
        "print(f\"Length of text: {len(text)} charecters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U2M_yu05NGE",
        "outputId": "5eab64d9-8562-421b-c58f-f75208cb6de7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 365071 charecters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique charecters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f\"{len(vocab)} unique charecters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi1X-wX16M3b",
        "outputId": "83b63fab-9abd-4805-dd60-35dc710f682d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "102 unique charecters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab),\n",
        "                                              mask_token=None)\n",
        "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(),\n",
        "                                              invert=True,\n",
        "                                              mask_token=None)"
      ],
      "metadata": {
        "id": "TS_cTRuC5ySp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "xbLQFGqK6V_K"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text,\n",
        "                                                  \"UTF-8\"))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13pKwTF96dSH",
        "outputId": "7d6f151c-40c0-440b-8d78-ea5baa1b80cc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(365071,), dtype=int64, numpy=array([102,  57,  33, ...,  16,  16,  16])>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "oG7sh0SZ6d6e"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "ojeE5PTx6kpf"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1,\n",
        "                              drop_remainder=True)"
      ],
      "metadata": {
        "id": "R7Kbc41B6oMG"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text"
      ],
      "metadata": {
        "id": "0GzbYcC26scI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "vsulpQqA6wEi"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "dataset = (dataset\n",
        "           .shuffle(BUFFER_SIZE)\n",
        "           .batch(BATCH_SIZE, drop_remainder=True)\n",
        "           .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTSSGSV861ZP",
        "outputId": "1e0e38e2-645e-4ec2-f062-c5940b11e552"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "bSZg-Ehd63Mk"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size,\n",
        "                                               embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    \n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "9z1bxrVZ65p-"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "aZb_Wuxb68pP"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "0V-nT2446-6b"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "                       embedding_dim=embedding_dim,\n",
        "                       rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "4Pgz7pCo7AHx"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss=loss)"
      ],
      "metadata": {
        "id": "Pjy4knmv7BeR"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = \"/content/drive/MyDrive/Kanye_logs\"\n",
        "# Name of the checkpoint file\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "z-BTXZbD7DgV"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 30"
      ],
      "metadata": {
        "id": "41KPhMv47GpL"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3KxuG3L7MuY",
        "outputId": "c0b696f9-60ac-43b8-de64-cf5765339ba0"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "56/56 [==============================] - 9s 82ms/step - loss: 3.5231\n",
            "Epoch 2/30\n",
            "56/56 [==============================] - 4s 67ms/step - loss: 2.4571\n",
            "Epoch 3/30\n",
            "56/56 [==============================] - 4s 68ms/step - loss: 2.2371\n",
            "Epoch 4/30\n",
            "56/56 [==============================] - 4s 65ms/step - loss: 2.0751\n",
            "Epoch 5/30\n",
            "56/56 [==============================] - 4s 64ms/step - loss: 1.9234\n",
            "Epoch 6/30\n",
            "56/56 [==============================] - 4s 65ms/step - loss: 1.8013\n",
            "Epoch 7/30\n",
            "56/56 [==============================] - 4s 70ms/step - loss: 1.6969\n",
            "Epoch 8/30\n",
            "56/56 [==============================] - 4s 64ms/step - loss: 1.6040\n",
            "Epoch 9/30\n",
            "56/56 [==============================] - 4s 65ms/step - loss: 1.5240\n",
            "Epoch 10/30\n",
            "56/56 [==============================] - 4s 63ms/step - loss: 1.4503\n",
            "Epoch 11/30\n",
            "56/56 [==============================] - 4s 63ms/step - loss: 1.3778\n",
            "Epoch 12/30\n",
            "56/56 [==============================] - 4s 62ms/step - loss: 1.3108\n",
            "Epoch 13/30\n",
            "56/56 [==============================] - 4s 62ms/step - loss: 1.2452\n",
            "Epoch 14/30\n",
            "56/56 [==============================] - 4s 63ms/step - loss: 1.1798\n",
            "Epoch 15/30\n",
            "56/56 [==============================] - 4s 63ms/step - loss: 1.1118\n",
            "Epoch 16/30\n",
            "56/56 [==============================] - 4s 63ms/step - loss: 1.0474\n",
            "Epoch 17/30\n",
            "56/56 [==============================] - 4s 67ms/step - loss: 0.9747\n",
            "Epoch 18/30\n",
            "56/56 [==============================] - 4s 64ms/step - loss: 0.9040\n",
            "Epoch 19/30\n",
            "56/56 [==============================] - 4s 67ms/step - loss: 0.8310\n",
            "Epoch 20/30\n",
            "56/56 [==============================] - 4s 70ms/step - loss: 0.7564\n",
            "Epoch 21/30\n",
            "56/56 [==============================] - 4s 63ms/step - loss: 0.6824\n",
            "Epoch 22/30\n",
            "56/56 [==============================] - 4s 68ms/step - loss: 0.6072\n",
            "Epoch 23/30\n",
            "56/56 [==============================] - 4s 67ms/step - loss: 0.5343\n",
            "Epoch 24/30\n",
            "56/56 [==============================] - 4s 69ms/step - loss: 0.4639\n",
            "Epoch 25/30\n",
            "56/56 [==============================] - 4s 65ms/step - loss: 0.4026\n",
            "Epoch 26/30\n",
            "56/56 [==============================] - 4s 72ms/step - loss: 0.3458\n",
            "Epoch 27/30\n",
            "56/56 [==============================] - 4s 63ms/step - loss: 0.2984\n",
            "Epoch 28/30\n",
            "56/56 [==============================] - 4s 69ms/step - loss: 0.2599\n",
            "Epoch 29/30\n",
            "56/56 [==============================] - 4s 71ms/step - loss: 0.2262\n",
            "Epoch 30/30\n",
            "56/56 [==============================] - 4s 62ms/step - loss: 0.1977\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2d2030fdc0>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated\n",
        "    skip_ids = self.ids_from_chars([\"[UNK]\"])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index\n",
        "        values =[-float(\"inf\")]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape of the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs\n",
        "    input_chars = tf.strings.unicode_split(inputs, \"UTF-8\")\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model\n",
        "    # predicted_logits.shape = [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids,\n",
        "                                          states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction ma: prevent \"[UNK]\" from being generated\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs\n",
        "    predicted_ids = tf.random.categorical(predicted_logits,\n",
        "                                          num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids,\n",
        "                               axis=-1)\n",
        "    \n",
        "    # Convert from token ids to charecters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the charecters and model state\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "8J_gFDUX7Nyj"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model,\n",
        "                         chars_from_ids,\n",
        "                         ids_from_chars)"
      ],
      "metadata": {
        "id": "DoXZeEA97tLj"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"[Intro]\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1200):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char,\n",
        "                                                       states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbMMkNCV7vFP",
        "outputId": "bf6633b4-8dba-42aa-871f-f23adda11b74"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Intro]\r\n",
            "You popitom, worgh if they'll he's bust there\r\n",
            "Everything I wanted, man it supmons\r\n",
            "I got play that forever when one of the Lood com\r\n",
            "That's high lights (Will hem edopi, baby, we major)\r\n",
            "We major? (C'mon, homie, we major)\r\n",
            "We major?\r\n",
            "\r\n",
            "\r\n",
            "[Chorus: Kanye West & The Game]\r\n",
            "Close unlems?\r\n",
            "You know the family cryin'? A loost by might and Have yauses\r\n",
            "(What it means)\r\n",
            "Somebody need to pum that that doom is like a man, shoulday\r\n",
            "So if you had her too, it don't affect me in the first\r\n",
            "But I am nothing like they never ever let your words post\r\n",
            "Man I, the kids was like Pick Solo\r\n",
            "Oh you're kidains from the same ol'?\r\n",
            "The God Capitol packs of the lights\r\n",
            "How many cars do we own?000 LL BkNow of God car\r\n",
            "Sharter that's purty Dame with mine to make them stays on my diamond-encrote man\r\n",
            "Oh, IG, I got your love lockdown\r\n",
            "Keeping your love lockdown—your love lockdown\r\n",
            "Now keep your love lockdown—you lose\r\n",
            "\r\n",
            "\r\n",
            "[Outro]\r\n",
            "You lose, you never knew\r\n",
            "While I'm drol-o, any\r\n",
            "You wave your daughter of their fresh\r\n",
            "But I can can't thing that you're a part of them family, in the day I give my manage is here\r\n",
            "More then, it's like T, at the morning, spit my wip, he all yeah\r\n",
            "And nothing hurts anymore, I feel k \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.309675455093384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's pretty cringe how the text often doesn't make much sense. To combat this I think it would be crucial to implement a module that would only allow for words from a certain set, so that nonense words could be skipped. It would also be an interesting project to introduce structure of the form [INTRO], [VERSE #], ..., [CHORUS], [VERSE #], ..., [OUTRO] so that it could create actual songs."
      ],
      "metadata": {
        "id": "-YWJ_iOlEmKU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5P17Q0O_FR-c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}